# -*- coding: utf-8 -*-
"""nnplayground.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XsKQBflyluO1bXo7Hj53KOzp3Mbix7u8

Ref:
https://medium.com/@elbasheer/building-a-multi-class-classification-model-with-pytorch-a-step-by-step-guide-747453f0f52a
https://towardsdatascience.com/the-complete-guide-to-neural-networks-multinomial-classification-4fe88bde7839
https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/
https://towardsdatascience.com/loss-functions-and-their-use-in-neural-networks-a470e703f1e9

https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html
https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid
https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss
"""

# General library imports
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt

# Neural Network imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Feed Forward Neural Network loading classified data where
# 0 - Move Forward
# 1 - Sharp-Right-Turn
# 2 - Slight-Right-Turn
# 3 - Slight-Left-Turn
# This was done since it error was thrown own while trying to read string using 'np.loadtxt()'
dataset = np.loadtxt('data_classified.csv', delimiter=',')
X = dataset[:,0:24]
y = dataset[:,24]

# Splitting data for training and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=23)

# Creating tensor out of numpy arrays
# X_train = torch.tensor(X_train, dtype=torch.float32)
# X_test = torch.tensor(X_test, dtype=torch.float32)
# y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)
# y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)
X_train = torch.from_numpy(X_train).type(torch.FloatTensor)
X_test = torch.from_numpy(X_test).type(torch.FloatTensor)
y_train = torch.from_numpy(y_train).type(torch.LongTensor)
y_test = torch.from_numpy(y_test).type(torch.LongTensor)

model = nn.Sequential(
    nn.Linear(24, 96),                                                            # Input layer with 24 features, hidden layer with 96 units
    # nn.Sigmoid(),                                                                 # Activation function
    nn.ReLU(),
    nn.Linear(96, 12),                                                            # Hidden layer with 12 units
    # nn.Sigmoid(),
    nn.ReLU(),
    nn.Linear(12, 4),                                                             # Output layer with 4 units (4 classes)
    nn.LogSoftmax()
)

# Optimizer function and Loss functon
loss_fn = nn.CrossEntropyLoss()                                                   # Cross Categorical Cross entropy
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Training the model
n_epochs = 100
batch_size = 20

# Create a DataLoader to handle batching for training
train_data = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

# Plot variables
average_loss = []

for epoch in range(n_epochs):
    epoch_loss = 0
    for Xbatch, ybatch in train_loader:

        optimizer.zero_grad()

        y_pred_train = model(Xbatch)

        loss = loss_fn(y_pred_train, ybatch)

        loss.backward()

        optimizer.step()

        epoch_loss += loss.item()

    print(f'Finished epoch {epoch+1}, average loss {epoch_loss / len(train_loader)}, latest loss {loss}')
    average_loss.append(epoch_loss / len(train_loader))

# Evaluating the model
# Disable gradient calculation as we are in evaluation mode, which reduces memory consumption and speeds up computations.
with torch.no_grad():
    # Perform a forward pass through the model using the test data.
    # y_test_hat_softmax contains the log-softmax output of the model.
    y_test_hat_softmax = model(X_test)

    # Use torch.max to get the predicted class labels.
    # y_test_hat is a tuple where the first element is the max value (softmax probability)
    # and the second element (indices) is the predicted class index.
    y_test_hat = torch.max(y_test_hat_softmax.data, 1)

# Convert tensors to numpy arrays for compatibility with scikit-learn's accuracy_score function.
y_test_np = y_test                                                                # Assuming y_test is already a numpy array, no conversion is needed here.
y_test_hat_np = y_test_hat.indices.numpy()                                        # Convert the predicted class indices tensor to a numpy array.

# Calculate and print the accuracy of the model on the test set.
output_classes = ["Move Forward", "Sharp Right Turn", "Slight Right Turn", "Slight Left Turn"]
accuracy = accuracy_score(y_test_np, y_test_hat_np)
precision = precision_score(y_test_np, y_test_hat_np, average=None)
recall = recall_score(y_test_np, y_test_hat_np, average=None)
print(f"Model Accuracy: {accuracy * 100:.2f}%")
for i in range(4):
  print(output_classes[i])
  print(f"Precision: {precision[i]}")
  print(f"Recall: {recall[i]}")

# Create the plot
n_epoch = list(range(1, 101))
plt.figure(figsize=(10, 6))
plt.plot(n_epoch, average_loss, linestyle='-', color='b')

# Adding titles and labels
plt.title('Average Loss vs. Number of Epochs')
plt.xlabel('Number of Epochs')
plt.ylabel('Average Loss')
# plt.xticks(n_epoch)  # Set x-ticks to be the epochs
plt.grid(True)

# Show the plot
plt.show()

# Calculate and print the accuracy of the model on the test set.
output_classes = ["Move Forward", "Sharp Right Turn", "Slight Right Turn", "Slight Left Turn"]
accuracy = accuracy_score(y_test_np, y_test_hat_np)
precision = precision_score(y_test_np, y_test_hat_np, average=None)
recall = recall_score(y_test_np, y_test_hat_np, average=None)
print(f"Model Accuracy: {accuracy * 100:.2f}%")
for i in range(4):
  print(output_classes[i])
  print(f"Precision: {precision[i] * 100:.2f}%")
  print(f"Recall: {recall[i] * 100:.2f}%")

